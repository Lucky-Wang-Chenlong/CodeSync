# DataProcessor

This directory provides an implementation of `CodeSync`. The pipeline of **CodeSync** consists of **4** parts: 

- **Step 1**: Real-World API Update Tracking
- **Step 2**: Real-World API Inovacation Retrieval
- **Step 3**: Legacy-Updated API Invocation Synthesis
- **Step 4**: Constructing a Benchmark for Real-Time Code Knowledge Assessment

## Function Illustration

The **entry function** of **CodeSync** is defined in file [pipeline.py](pipeline.py). This function support execute each step independently:

- **Step 1**: This part is implementated in file [API update](api_update.py). **CodeSync** would retrieve API updates automatically according to [config file](../hparams/configs/dataset_config.yml) when executing `CodeSync`.
- **Step 2**: This part is time consuming. The processed dataset would be save into `config.raw_data_dir`. This part is divided into **3** independent parts, but we suggest to set **True** simultaneously:
  - **crawling**: Program can crawl API invocation files from GitHub based on API updates collected by **Step 1**.
  - **api_extractor**: This part will identify API invocations from config.libs and reorganize the data into jsonl files, with each file corresponding to a specific API. 
  - **data_filter**: Based on files processed by **api_extractor**, this part will filter out unchanged API according to the result of **Step 1**.
- **Step 3**: This part is to prompt LLM API to synthesize lagacy / updated API invocations and convert to MetaData format, which can be used as training data.
- **Step 4**: This part is to contruct benchmark based on metadata generated by **Step 3**.


You can execute **CodeSync** by executing [bash script](codesync.sh):
```bash
codesync.sh --crawling --filter --synthesis --benchmark
```
or executing [python script](pipeline.py):
```bash
pipeline.py --crawling True --filter True --synthesis True --benchmark True
```

## Config File

The configuration file is saved in [dataset_config.yml](../hparams/configs/dataset_config.yml).

1. ***Libaray Configuration*** 
   - **libs**: **List[str]**. List the names of libraries that you want to explore. 
     - For example, ['torch', 'scikit-learn']
   - **lib_names**: **List[str]**. List the corresponding name of libraries in Python program. This is because in Python, the names of libraries used for installation and import can differ
     - For example, `pip install scikit-learn` -> `import sklearn`
   - **versions**: **List[List[str]]**. Each item is a list containg 2 string, representing the outdated and updated versions respectively.

2. ***Saving Path Configuration***
   - **raw_data_dir**: Saving path of [Step 2](#function-illustration).
   - **data_dir**: Saving path of [Step 3](#function-illustration).
   - **benchmark_dir**: Saving path of [Step 4](#function-illustration).

3. ***Crawling Configuration***
   - **token**: List[str]. GitHub tokens for crawling files from GitHub.

4. ***LLM Configuration***
   This part is compatible with `OpenAI`
   - **llm_api**: API key of calling LLM.
   - **llm_name**: LLM name.
   - **llm_url**: Official url of selected LLM.

## P.S.
For each library, [Step 1](#function-illustration) involes setting up **virtual environments** and installing appropriate version of target library (either outdated or updated). The program can automatically create venv in a directory like [venv/xxx](venv), for example, `venv/torch-2.5.0`. 

However, automatic venv creation might fail due to package dependency conflicts. We recommend that users manually set up venv to avoid these issues.
